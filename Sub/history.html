<html>
<head>
  <meta charset ="UTF-8">
  <title>Lịch sử phát triển NLP</title>
  <link rel="icon" href="fav.ico" />
 </head>
 <body>
    <h1> <u><i><b>Lịch sử </b></i></u> </h1>
	<table>
	<tr>
	<td>
    <ul>
	 <li> <b> 1940s-1950s: </b>  Đây là giai đọan của những viên gạch đầu tiên về NLP với  <br/> nền tảng:
         	 máy tự động và mô hình về xác suất hay thông tin lý thuyết.Máy  <br/>tự động nổi lên trong những năm 50 bắt nguồn từ mô hình tính toán thuật <br/> toán của Alan Turing, được nhiều người cho là nền tảng của khoa học máy  <br/>tính hiện đại.Shannon (1948) áp dụng mô hình xác suất của các quá trình  <br/>rời rạc Markov vào máy tự động cho ngôn ngữ. Năm 1950 Alan Turing <br/> xuất bản bài viết "Computing Machinery and Intelligence" để đề xuất một <br/> tiêu chuẩn đánh giá trí thông minh của máy tính, ngày nay gọi là "Turing  <br/>test". Giai đoạn này đã xuất hiện máy dịch thuật (MT)- máy tính đầu  <br/>tiên dựa trên ứng dụng liên quan đến xử lí ngôn ngữ tự nhiên.Công việc <br/> ban đầu của MT chỉ đơn giản là chuyển đổi giữa các ngôn ngữ khác nhau  <br/>dựa trên từ vựng - với quan điểm rất đơn giản là tra cứu từ điển và sắp <br/> xếp lại cho phù hợp với quy tắc về thứ tự của ngôn ngữ đó - mà bỏ qua <br/> nhiều vấn đề như sự nhập nhằng của ngôn ngữ,.. 
		</ul>
		<center><img src="turing-test.jpg" width="304" height="342"></center>
    </td>
    <td>
     <ul>
         <li> <b> 1957-1970s:</b> Đến cuối những năm 50 và đầu những năm 60, xử lí lời nói<br/> và ngôn ngữ đã chia ra rất rõ ràng thành hai loại: symbolic (biểu tượng)<br/> và stochastic (ngẫu nhiên)...Symbolic được bắt đầu từ hai hướng nghiên <br/>cứu. Đầu tiên là công trình của Chomsky và những người khác về lý luận <br/>về cú pháp ngôn ngữ hình thức và "cú pháp biến tạo" (generative syntax)<br/> trong cuối những năm 50 đến giữa thập niên 60, và công trình của những <br/>nhà ngôn ngữ học và khoa học máy tính về phân tích thuật toán. Hướng<br/> nghiên cứu còn lại là một lĩnh vực mới: trí tuệ nhân tạo. Vào mùa hè<br/> năm 1956 John McCarthy, Marvin Minsky, Claude Shannon và Nathaniel <br/>Rochester cùng một nhóm các nhà nghiên cứu đã tổ chức một hội thảo<br/> kéo dài hai tháng về thứ mà họ đã quyết định gọi là trí tuệ nhân tạo (AI).<br/> Stochastic được sử dụng chủ yếu trong ngành thống kê và kĩ thuật điện <br/>(electrical engineering) .Vào cuối thập kỉ 50, phương pháp Bayesian đã <br/>bắt đầu được áp dụng cho nhận dạng kí tự.	 
		</li>
	</ul>
	  <center><img src="SS.jpg" width="304" height="342"></center>
	</td>
	</tr>
    </table>
	<br/>
	<table>
	<tr>
	<td>
	<ul>
	<li> Những bất cập tồn tại trong hệ thống dẫn đến ALPAC (Automatic Language <br/>Processing Advisory Committee của Viện Hàn lâm khoa học – Hội đồng <br/>nghiên cứu quốc gia) năm 1966. Báo cáo kết luận rằng MT đã không thể<br/> đáp ứng những dự tính/ dự định đặt ra từ trước, dẫn đến những dự án <br/>MT chỉ còn được tài trợ rất ít. Những nghiên cứu về MT chỉ được tiếp tục <br/>rất lâu sau đó (cuối những năm 80) khi hệ thống dịch máy thống kê <br/>đầu tiên (statistical machine translation systems) được tạo ra. Các công trình <br/>nghiên cứu lý thuyết từ cuối 1960 đến đầu 1970 tập trung vào vấn đề <br/> diễn giải nghĩa và tạo ra những hệ thống tính toán với nhiều phương pháp <br/>khác nhau. Cùng với phát triển lý thuyết,nhiều mẫu thử nghiệm hệ thống <br/>được phát triển để chứng minh hiệu quả của các nguyên tắc cụ thể. Tiêu <br/>biểu nhất là Eliza - một trong những chatterbot đầu tiên, và cũng là<br/> phần mềm đầu tiên có thể vượt qua được Turing test.
	</li>
	</ul>
	<center> <img src="eliza.jpg" width="304" height="342"></center>
	</td>
	<td>
	<ul>
	<li> <b>1970s:</b> Nhiều lập trình viên bắt đầu viết những "conceptual ontologies",<br/> thứ cấu trúc lại những thông tin thực tế thành những dữ liệu mà máy tính <br/>có thể hiểu được. Cũng trong khoảng thời gian này, nhiều chatterbots được <br/>viết bao gồm PARRY, Racter,và Jabberwacky.
	</li>
	<li> <b> 1980s-1993 </b>  Cho đến thập niên 80, hầu hết các hệ thống NLP được dựa<br/>  trên những bộ quy tắc phức tạp được viết tay. Bắt đầu từ cuối những năm <br/> 80 nhưng thật sự đã có một cuộc cách mạng trong NLP với sự ra đời của <br/> thuật toán Machine learning cho xử lí ngôn ngữ. Điều này có được là nhờ <br/> cả sự tăng ổn định về năng lực tính toán của máy tính và sự thống trị bị <br/> giảm dần của các lý thuyết về ngôn ngữ học của Chomsky - thứ mà nền <br/> tảng chỉ là lý thuyết suông chống lại Corpus Linguistics - nền tảng cho <br/> mối quan hệ của Machine learning và xử lí ngôn ngữ - cùng những gì liên<br/>  quan.Những thuật toán ML đầu tiên được sử dụng, như Decision Tree, <br/> tạo ra những hệ thống với những quy tắc về if-then hết sức cứng nhắc -<br/>  tương tự như những quy định về viết tay cùng tồn tại lúc đó.
	</ul>
	<center><img src="ml.jpg" width="244" height="272"></center>
	</td>
	</tr>
	</table>
	<ul>
	<li> <b> 1994 cho tới nay.</b> Trong 5 năm cuối của thiên niên kỉ, có thể thấy sự thay đổi rõ rệt của NLP. Thứ nhất là những mô hình thống kê và điều khiển bằng dữ liệu (probabilistic and data-driven models) đã trở thành những tiêu chuẩn của NLP. Thứ 2 là sự phát triển về tốc độ cũng như bộ nhớ của máy tính đã mở ra sự khai thác về mặt kinh tế những subarea của NLP như nhận dạng ngôn ngữ hay kiểm tra chính tả và ngữ pháp. Có 3 xu hướng chính thúc đẩy sự phát triển của NLP: (1) Một lượng lớn dữ liệu về tài liệu nói và viết trở nên phổ biến dưới sự bảo trợ của Linguistic Data Consortium (LDC) và những tổ chức tương tự khác. Những nguồn tài nguyên này đã góp phần thúc đẩy xu hướng xử lí những vấn đề truyền thống nhưng phức tạp, như phân tích ngữ pháp hay ngữ nghĩa, cũng như những vấn đề về supervised machine learning; (2) Sự chú ý vào nghiên cứu được gia tăng dẫn đến những sự tác động qua lại ngày càng mật thiết với cộng đồng Học máy và thống kê. (3) Cuối cùng là sự phổ biến của những hê thống điện toán với hiệu suất cao (high-performance computing systems) giúp cho việc hướng dẫn và triển khai các hệ thống NLP được đơn giản hơn.
	</li>
	</ul>
	<center> <img src="hpc.png" width="304" height="342"></center>
 </body>
 </html>